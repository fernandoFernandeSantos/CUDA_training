
//
//
//int gemm(float** a, float** b, float** c, long lin_a, long col_a, long lin_b,
//		long col_b) {
//	long i, j, k;
//	if (col_a != lin_b)
//		return -1;
//	for (i = 0; i < lin_a; i++)
//		for (j = 0; j < col_b; j++) {
//			c[i][j] = 0;
//			for (k = 0; k < col_a; k++)
//				c[i][j] += a[i][k] * b[k][j];
//		}
//	return 0;
//}
//
//int gemm_1d(float* a, float* b, float* c, long lin_a, long col_a, long lin_b,
//		long col_b, long col_c, long lin_c) {
//	long i, j, k;
//	if (col_a != lin_b)
//		return -1;
//
//	for (i = 0; i < lin_a; i++) {
//		for (j = 0; j < col_b; j++) {
//			long index_c = i * col_c + j;
//			c[index_c] = 0;
//			for (k = 0; k < col_a; k++) {
//				c[index_c] += a[i * col_a + k] * b[k * col_b + j];
//			}
//		}
//		//printf("\n");
//	}
//	return 0;
//}
//
//__global__ void mat_cpy(float *dst, float *src, long collums, long rows) {
//	long x = (blockDim.x * blockIdx.x) + threadIdx.x;
//	long y = (blockDim.y * blockIdx.y) + threadIdx.y;
//
//	long index = (collums * y) + x;
//
//	if (collums * rows > index)
//		dst[index] = src[index];
//}
//int gemm_ongpu_abft(float *a, float *b, float *c, long lin_a, long col_a,
//		long lin_b, long col_b) {
//	long i, j;
//	float acc = 0;
//	int ret = 0;
//	long col_c = col_b;
////	long lin_c = lin_a;
//	//first ABRAHAM operation
//	for (j = 0; j < col_a; j++) {
//		acc = 0;
//		for (i = 0; i < lin_a; i++)
//
//			acc += a[i * col_a + j];
//
//		a[lin_a * col_a + j] = acc;
//	}
//
//	//second ABRAHAM operation
//	for (i = 0; i < lin_b; i++) {
//		acc = 0;
//		for (j = 0; j < col_b; j++)
//			acc += b[i * (col_b + 1) + j];
//		//printf("i * col_b %ld col b %ld  acc %lf\n", i * col_b, col_b, acc);
//		b[i * (col_b + 1) + col_b] = acc;
//	}
//
//	//print_mat(a, lin_a + 1, col_a);
//	//printf("\n");
//	//print_mat(b, lin_b, col_b + 1);
//	//performs matrix multiplication
//	gemm_1d(a, b, c, lin_a + 1, col_a, lin_b, col_b + 1, col_b + 1, lin_a + 1);
//
//	//check all checksums
//	//line checksum
//	for (j = 0; j < col_a; j++) {
//		acc = 0;
//		for (i = 0; i < lin_a; i++)
//			acc += c[i * col_c + j];
//
//		if (fabs(c[lin_a * col_c + j]) - fabs(acc) >= MAX_THRESHOLD) {
////			printf(
////					"lin - position corrupted [%ld][%ld] - exp chsum %lf got chsum %lf diff - %lf\n",
////					lin_a, j, c[lin_a * col_c + j], acc,
////					c[lin_a * col_c + j] - acc);
//			ret++;
//		}
//	}
//
//	//collum checksum
//	for (i = 0; i < lin_b; i++) {
//		acc = 0;
//		for (j = 0; j < col_b; j++)
//			acc += c[i * col_c + j];
//
//		if (fabs(c[i * col_c + col_b] - acc) >= MAX_THRESHOLD) {
////			printf(
////					"collum - position corrupted [%ld][%ld] - exp chsum %lf got chsum %lf diff %lf\n",
////					i, col_b, c[i * col_c + col_b], acc,
////					c[i * col_c + col_b] - acc);
//			ret++;
//		}
//	}
//	return ret;
//
//}
//
//void matrix_multiplication_no_abft() {
//	const long siz_a = VECTOR_SIZE_A * sizeof(float);
//	const long siz_b = VECTOR_SIZE_B * sizeof(float);
//	const long siz_c = VECTOR_SIZE_C * sizeof(float);
//	//host memories
//	float* host_array_a = (float*) calloc(VECTOR_SIZE_A, sizeof(float));
//	float* host_array_b = (float*) calloc(VECTOR_SIZE_B, sizeof(float));
//	float* host_array_c = (float*) calloc(VECTOR_SIZE_C, sizeof(float));
//	float* host_array_c_temp = (float*) calloc(VECTOR_SIZE_C, sizeof(float));
//	fill_mat(host_array_a, VECTOR_SIZE_A);
//	fill_mat(host_array_b, VECTOR_SIZE_B);
//	//print_mat(host_array_a, COLLUMS_A, ROWS_A, "matrix A");
//	printf("\n");
//	//print_mat(host_array_b, COLLUMS_B, ROWS_B, "matrix B");
//	//perform host matrix multiplication
//	//	gemm_1d(host_array_a, host_array_b, host_array_c_temp, ROWS_A, COLLUMS_A,
//	//			ROWS_B, COLLUMS_B, ROWS_A, COLLUMS_B);
//	//print_mat(host_array_c_temp, COLLUMS_B, ROWS_A, "matrix C temp");
//	//cuda memories
//	float *device_array_a, *device_array_b, *device_array_c;
//	cudaMalloc(&device_array_a, siz_a);
//	cudaMalloc(&device_array_b, siz_b);
//	cudaMalloc(&device_array_c, siz_c);
//	//copy to device
//	cudaMemcpy(device_array_a, host_array_a, siz_a, cudaMemcpyHostToDevice);
//	cudaMemcpy(device_array_b, host_array_b, siz_b, cudaMemcpyHostToDevice);
//	//kernel parameters
//	//we know that each block has 1024 threads
//	long blocks = ceil(N / float(BLOCK_SIZE));
//	long threads = ceil(N / float(blocks));
//	//2d grid
//	dim3 gridDim(blocks, blocks);
//	//threads num, 2d
//	dim3 blockDim(threads, threads);
//	mat_mult<<<gridDim, blockDim>>>(device_array_c, device_array_a,
//			device_array_b, N);
//	printf("\nblocks %ld threads %ld\n", blocks, threads);
//	cudaMemcpy(host_array_c, device_array_c, siz_c, cudaMemcpyDeviceToHost);
//	//print_mat(host_array_c, COLLUMS_A, ROWS_A, "GPU result mat");
//	printf("compare matrices\n");
//	//compare(host_array_c, host_array_c_temp, VECTOR_SIZE_C);
//	cudaFree(device_array_a);
//	cudaFree(device_array_b);
//	cudaFree(device_array_c);
//	free(host_array_a);
//	free(host_array_b);
//	free(host_array_c);
//	free(host_array_c_temp);
//}
//
//__global__ void mat_mult(float *dst, float *a, float *b, long col) {
//	long i = blockIdx.y * blockDim.y + threadIdx.y;
//	long j = blockIdx.x * blockDim.x + threadIdx.x;
//
//	if (i > col || j > col)
//		return;
//
//	float acc = 0;
//	long index_dst = i * col + j;
//	long k;
//	for (k = 0; k < col; k++) {
//		acc += a[i * col + k] * b[k * col + j];
//	}
//	dst[index_dst] = acc;
//}
__global__ void cp_row(float *dst, float *src, long rows, long cols){
	long j = blockIdx.x * blockDim.x + threadIdx.x;
	long a_index = (rows - 1) * cols + j;
	//printf("a_index %ld acc %lf \n", a_index, acc);
	dst[j] = src[a_index];
}

__global__ void cp_col(float *dst, float *src, long row, long cols){
	long i = blockIdx.x * blockDim.x + threadIdx.x;
	long b_index = i * cols + cols - 1;
	dst[i] = src[b_index];
}

__global__ void save_vectors(float *a, float *b, long rows_a, long cols_a,
		long rows_b, long cols_b, float *saved_row, float *saved_col) {
	long i = blockIdx.x * blockDim.x + threadIdx.x;
	//printf("i value %ld\n", i);
	//rows
	if (i == 0) {
		//1d grid for abft operations
		long blocks = ceil(cols_a / float(BLOCK_SIZE));
		long threads = ceil(cols_a / float(blocks));
		cp_row<<<blocks, threads>>>(saved_row, a, rows_a, cols_a);
	}

	if (i == 1) {
		//second
		long blocks = ceil(rows_b / float(BLOCK_SIZE));
		long threads = ceil(rows_b / float(blocks));
		cp_col<<<blocks, threads>>>(saved_col, b, rows_b, cols_b);
	}
	__syncthreads();
}

__device__ long get_index(long i, long j, long rows, long cols,
		char major = 'r') {
	if (major == 'r')
		return i * cols + j;
	return i * rows + j;
}


__global__ void copy_and_add(float *dst, float *src, long rows, long cols,
		char major = 'r') {
	long j = blockIdx.x * blockDim.x + threadIdx.x;
	long i = blockIdx.y * blockDim.y + threadIdx.y;

	if (major == 'r') {
		long index = get_index(i, j, rows, cols);
		if (index % rows) {
			dst[index] = src[index];
		} else if (index != 0) {
			dst[index] = 0;
		}

		if ((rows * cols) < index){
			dst[index] = 0;
		}
	}else{
		//not implemented yet
	}

}
//DYNAMIC PARALLELISM ONLY TO CALL NEW KERNELS, ARE FUCK KIDDING???
//man, I am so lazy
__global__ void check_checksums(float *c, long rows_c, long cols_c) {
	long i = blockIdx.x * blockDim.x + threadIdx.x;
	//printf("i value %ld\n", i);
	//rows
	if (i == 0) {
		long blocks = ceil(float(cols_c) / float(BLOCK_SIZE));
		long threads = ceil(float(cols_c) / float(blocks));
		check_row<<<blocks, threads>>>(c, rows_c, cols_c);
//		printf("cols %d blocks %ld threads %ld\n", cols_c, blocks, threads);
	}
	//cols
	if (i == 1) {
		long blocks = ceil(float(rows_c) / float(BLOCK_SIZE));
		long threads = ceil(float(rows_c) / float(blocks));
		check_col<<<blocks, threads>>>(c, rows_c, cols_c);
//		printf("blocks %ld threads %ld\n", blocks, threads);
	}
	//printf("passou aqui foi\n");

	__syncthreads();
	//printf("values %d %d\n ", row_detected_errors, col_detected_errors);
}

__global__ void calc_checksums(float *a, float *b, long rows_a, long cols_a,
		long rows_b, long cols_b) {
	long i = blockIdx.x * blockDim.x + threadIdx.x;
	//printf("i value %ld\n", i);
	//rows
	if (i == 0) {
		//1d grid for abft operations
		long blocks = ceil(float(cols_a) / float(BLOCK_SIZE));
		long threads = ceil(float(cols_a) / float(blocks));
		first_abraham_op<<<blocks, threads>>>(a, rows_a, cols_a);
	}

	if (i == 1) {
		//second
		long blocks = ceil(float(rows_b) / float(BLOCK_SIZE));
		long threads = ceil(float(rows_b) / float(blocks));
		second_abraham_op<<<blocks, threads>>>(b, rows_b, cols_b);
	}
	__syncthreads();
}

void abraham_sum(float *a, float *b, long rows_a, long cols_a, long rows_b,
		long cols_b) {

	calc_checksums<<<1, 2>>>(a, b, rows_a, cols_a, rows_b, cols_b);
	gpuErrchk(cudaPeekAtLastError());
}


void abraham_check(float *c, long rows, long cols) {
	check_checksums<<<1, 2>>>(c, rows, cols);
	gpuErrchk(cudaPeekAtLastError());
}

void print_mat_collum_major(float *mat, long m, long n, const char *mat_name) {
	printf("COLLUM-MAJOR ORDER: printing %s lin %ld col %ld\n", mat_name, m, n);
	long i, j;
	for (i = 0; i < m; i++) {

		for (j = 0; j < n; j++) {
			printf("%ld ", (PRINT_TYPE) mat[j * m + i]);
		}
		printf("\n");
	}
//	printf("on vector 1d\n");
//	for (i = 0; i < m * n; i++) {
//		printf("%ld ", (PRINT_TYPE) mat[i]);
//	}
	printf("\n");

}

void fill_mat_collum_major(float *t, long m, long n) {
	long i, j;
	for (i = 0; i < m; i++)
		for (j = 0; j < n; j++)
			t[j * m + i] = float(i);
}